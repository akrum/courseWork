\section{Математическая модель линейной регрессии с выбросами при наличии группирования наблюдений}
Рассмотрим модель линейной регрессии:
\begin{eqnarray}
    &\label{eq2}y_i= 
    \begin{pmatrix}
        \beta_0\\
        \beta_1\\
        \dots\\
        \beta_n
    \end{pmatrix}^{T}\times
    \begin{pmatrix}
        1\\
        x_{i1}\\
        \dots\\
        x_{in}
    \end{pmatrix}+ \varepsilon_i,\\
    &\label{eq6}y_i= f(x_i,\beta)+\varepsilon_i,\\
    &f(x_i,\beta)=\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+\dots+\beta_n x_{in}.
\end{eqnarray}
Здесь $y_i$ -- зависимая переменная, $x_i=(x_{i1},x_{i2},\dots,x_{in})$ -- вектор регрессоров, \{$\beta_k, k=\overline{0,n}$\}-- коэффициенты линейной регрессии, а $\varepsilon_i$ -- случайная ошибка $i$-го эксперимента, распределение которой подчиняется нормальному закону с нулевым математическим ожиданием и дисперсией $\sigma^2$, $N$ -- объем выборки.
Согласно (\ref{eq6}) каждый $y_i$ принадлежит нормальному распределению:
\begin{eqnarray}
    \label{eq12} y_i=f(x_i,\beta)+\varepsilon_i \sim \mathcal{N}(f(x_i,\beta),\sigma^2).
\end{eqnarray}

Предполагается, что выборка содержит выбросы, описываемые следующим соотношением.
\begin{eqnarray}
    \label{eq3}y_i^{\widetilde{\varepsilon}}=(\xi_i)y_i+ (1-\xi_i)\eta_i,
\end{eqnarray}
где $\xi_i$ принимает значение, равное 1, с вероятностью $1-\widetilde{\varepsilon}$ и значение, равное 0, с вероятностью $\widetilde{\varepsilon}$:
\begin{eqnarray}\label{eq4}
    \begin{cases}
        P\{\xi_i=0\}=\widetilde{\varepsilon},\\
        P\{\xi_i=1\}=1-\widetilde{\varepsilon},
    \end{cases}
\end{eqnarray}
$\eta_i$-случайная величина из некоторого вообще говоря неизвестного распределения.

Параметр $\xi_i$ имеет следующий содержательный смысл: если $\xi_i=0$, то вместо истинного значения мы наблюдаем выброс, если $\xi_i=1$, то наблюдается истинное значение.
Переменную $\widetilde{\varepsilon}$ будем называть долей аномальных наблюдений. Величины $\xi_i, x_i$ и $\eta_i$ являются независимыми.

Пусть множество значений функции регрессии, т.е множество $\mathbb{R}$, разбито на $k$ непересекающихся полуинтервалов:
\begin{eqnarray}
    \mathbb{R}=(-\infty,a_1]\bigcup(a_1,a_2]\bigcup \dots \bigcup(a_{k-1},+\infty ).
\end{eqnarray}
Полученные полуинтервалы будем обозначать: $\nu_0,\dots,\nu_{k-1}$.

Предполагается, что каждый раз вместо истинного значения зависимой переменной $y_i$ наблюдается только номер интервала, в который это наблюдение попало.
Тогда для каждого $y_i$ будем наблюдать лишь номер полуинтервала $\mu_i$, в который он попал.
\begin{eqnarray}
    \label{eq13}\mu_i=j, \textup{если $y_i$} \in \nu_j.
\end{eqnarray}

В таком случае принято говорить, что имеет место группирование наблюдений, а сами наблюдения называются группироваными \cite{OLSforGrouping}.

В работе рассматривается задача оценивания параметров линейной регрессии с аномальными наблюдениями при наличии группирования наблюдений.

\section{Способы оценивания параметров линейной регрессии с выбросами}
Для модели регрессии (\ref{eq2}), то есть для классической модели линейной регрессии, существует несколько способов оценивания параметров. Далее приведены некоторые из них.

\subsection{Метод наименьших квадратов}
Метод наименьших квадратов строится из предположения, что ошибки подчиняются нормальному закону распределения вероятностей:
\begin{eqnarray}
    \label{eq5}L\{\varepsilon_i\}=N_1(0,\sigma^2), i = \overline{1,n}.
\end{eqnarray}
Исходя из предположения (\ref{eq5}) можно построить логарифмическую функцию правдоподобия. В силу (\ref{eq6}) имеем:
\begin{eqnarray}
    L\{y_i\}=N_1(f(x_i;\beta), \sigma^2).
\end{eqnarray}
Логарифмическая функция правдоподобия выглядит так\cite{Kharin}:
\begin{eqnarray}
    l(\beta)=\ln \prod_{i=1}^{n}(\frac{1}{\sqrt{2\pi}\sigma}e^{-\frac{(y_i-f(x_i;\beta))^2}{2\sigma^2}})&=&-\frac{1}{2}n\ln{2\pi\sigma^2}-\frac{1}{2\sigma^2}R^2(\beta),\\
    \label{eq7}R^2(\beta)=\sum_{i=1}^{n}(\delta y_i)^2&=&\sum_{i=1}^{n}(y_i-f(x_i,\beta))^2\geq 0.
\end{eqnarray}
Оценка методом наименьших квадратов называется такая оценка, которая минимизирует  выражение (\ref{eq7}) такова:
\begin{eqnarray}
    \hat{\beta}=arg \min_{\beta}R^2(\beta).
\end{eqnarray}

Метод наименьших квадратов не считается робастным способом оценивания параметров линейной регрессии. Для иллюстрации этого был проведен компьютерный эксперимент (раздел \ref{ss_1}), в результате которого вариации оценок не уменьшались с увеличением объема выборки, в которой присутствовали аномальные наблюдения.
Это означает, что построенные оценки не являются состоятельными в случае, если в данных присутствуют выбросы.

\subsection{М-оценки}
М-оценки являются робастным способом оценивания параметров линейной регрессии с аномальными наблюдениями.
Швейцарский статистик П.Хьюбер предложил использовать М-оценки \cite{Kharin}, которые являются решениями экстремальных задач вида:
\begin{eqnarray}
    \sum_{i=1}^{n}\phi(x_t;\beta)\rightarrow \min_{\beta},
\end{eqnarray}
где $\phi(\cdot;\beta)$-некоторая функция, определяющая конкретный тип оценок и их точность.

Очевидно, что $\phi(\cdot;\beta)\equiv - \ln{p(\cdot;\beta)}$ дает обычную оценку максимального правдоподобия, построенную по модели без выбросов (\ref{eq2}).

Рассмотрим теперь некоторые способы выбора функции $\phi(\cdot;\beta)$ для решения экстремальной задачи в M-оценках.

Для начала определим:
\begin{eqnarray}
    u_i=y_i^{\widetilde{\varepsilon}}-(\beta_0+\beta_1 x_{i1}+\beta_2 x_{i2}+\dots+\beta_n x_{in}).
\end{eqnarray}
Тогда существует такие методы\cite{RobustRegression}:\hfill\break
\begin{center}
\begin{tabular}{ |p{3cm}|p{10cm} | }
    \hline
    \multicolumn{2}{|c|}{Способы выбора $\phi(\cdot;\beta)$} \\
    \hline
    Метод& Целевая функция\\
    \hline
    Метод Наименьших Квадратов&$\phi(\cdot;\beta)_{OLS}=u^2$\\
    \hline
    Хьюбера&$\phi(\cdot;\beta)_{H}=
        \begin{cases}
            \frac{1}{2}u^2, |u|\leq k,\\
            k|u|-\frac{1}{2}k^2, |u|>k
        \end{cases}$\\
    \hline
    Биквадратный& $\phi(\cdot;\beta)_{B}=
    \begin{cases}
        \frac{k^2}{6}(1-[1-(\frac{u}{k})^2]^3), |u|\leq k\\
        \frac{k^2}{6}, |u|>k
    \end{cases}$\\
    \hline
\end{tabular}
\end{center}

В разделе \ref{ss_1} приведено сравнение М-оценок с целевой функцией Хьюбера с оценками по Методу наименьших квадратов.
Сравнение показало отличие поведения вариаций робастных М-оценок (вариации уменьшались) от вариаций оценок МНК (вариации не уменьшались) при увеличении объема выборки с искажениями.

\newpage

\newpage
\section{Статистическое оценивание параметров линейной регрессии с выбросами при наличии группирования наблюдений}
\subsection{Оценки максимального правдоподобия параметров линейной регрессии при наличии группирования наблюдений}
Введем обозначение для функции распределения стандартного нормального закона:
\begin{eqnarray}
    \Phi(x)=\frac{1}{\sqrt{2}\sigma}\int_{-\infty}^{x}e^{\frac{-t^2}{2}}dt.
\end{eqnarray}

Тогда функцию распределения нормального закона с параметрами $\mu,\sigma^2$ можно представить как:
\begin{eqnarray}
    \label{eq10}F(x)=\Phi(\frac{x-\mu}{\sigma}),
\end{eqnarray}
где $\sigma = \sqrt{\sigma^2}$. \hfill\break
Обозначим:
\begin{eqnarray}
    \textup{erf}(x)=\frac{2}{\sqrt{\pi}}\int_0^{x}e^{-t^2}dt.
\end{eqnarray}
Тогда:
\begin{eqnarray}
    \Phi(x)= \frac{1}{2} \Big[1+\textup{erf}\Big(\frac{x}{\sqrt{2}} \Big) \Big].
\end{eqnarray}
Подставив полученные выражения в (\ref{eq10}) получим:
\begin{eqnarray}
    F(x)= \frac{1}{2} \Big[1+\textup{erf}\Big(\frac{x-\mu}{\sqrt{2}\sigma} \Big) \Big].
\end{eqnarray}
При модельных предположениях (\ref{eq12}) вероятность попадания $y_i$ в полуинтервал $\nu_j$ равна:
\begin{multline}
    P\{y_i\in\nu_j\}= F_{y_i}(a_{j+1})-F_{y_i}(a_{j})=\\
    =\begin{cases}
        \frac{1}{2}(\textup{erf}(\frac{a_{j+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{j}-f(x_i,\beta)}{\sqrt{2}\sigma})),~j=\overline{1,k-2}\\
        \frac{1}{2}(1+\textup{erf}(\frac{a_{1}-f(x_i,\beta)}{\sqrt{2}\sigma})),~j=0\\
        -\frac{1}{2}(1+\textup{erf}(\frac{a_{k-1}-f(x_i,\beta)}{\sqrt{2}\sigma})),~j=k-1
    \end{cases}.
\end{multline}
Понятно, что:
\begin{eqnarray}
    P(\mu_i=j)=P(y_i\in \nu_{\mu_i}).
\end{eqnarray}

Решается задача статистического оценивания параметров модели (\ref{eq2}) \{$\beta_k, k=\overline{0,n}$\} по известным группированным наблюдениям. Здесь наличие аномальных наблюдений не учитывается.

Для этого построим функцию правдоподобия:
\begin{eqnarray}
    \label{eq22}\label{eq23}&l(\beta,\sigma^2, \mu_1,\dots, \mu_{N})=\sum\limits_{i=1}^{N}\ln(P(y_i\in \nu_{\mu_i}))=\\
    &=\sum\limits_{i=1}^{N}\ln\begin{cases}
        \frac{1}{2}(\textup{erf}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma})),~\mu_i=\overline{1,k-2};\\
        \frac{1}{2}(1+\textup{erf}(\frac{a_{1}-f(x_i,\beta)}{\sqrt{2}\sigma})),~\mu_i=0;\\
        -\frac{1}{2}(1+\textup{erf}(\frac{a_{k-1}-f(x_i,\beta)}{\sqrt{2}\sigma})),~\mu_i=k-1,
    \end{cases}
\end{eqnarray}
где:
\begin{eqnarray}
    \textup{erf}(x)=\frac{2}{\sqrt{\pi}}\int_0^{x}e^{-t^2}dt.
\end{eqnarray}

Для максимизации функции правдоподобия решим систему уравнений:
\begin{eqnarray}
    \label{eq24}\frac{\delta l}{\delta \beta}=0_{n+1},
\end{eqnarray}
где:
\begin{multline}
    \label{eq27}\frac{\delta l}{\delta \beta}=\frac{\delta \sum\limits_{i=1}^{N}\ln P(y_i\in \nu_{\mu_i})}{\delta \beta}=~\\
    =\frac{\delta \sum\limits_{i=1}^{N} \ln(\frac{1}{2}(\textup{erf}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma})) )         }{\delta \beta}=~\\
    =  \sum\limits_{i=1}^{N}\Big((1-(\delta_{\mu_i 0}+\delta_{\mu_i k-1}))\frac{(\textup{erf'}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf'}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}{ (\textup{erf}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}+~\\
    +(\delta_{\mu_i 0}-\delta_{\mu_i k-1})\frac{\textup{erf'}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma})}{(1+\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}\Big)  (-1) \frac{\delta f(x_i,\beta)}{\delta \beta} )=~
\end{multline}
\begin{multline}
    \nonumber 
    =-\sum_{i=1}^{N}\begin{pmatrix}
        1\\
        x_{i1}\\
        \dots\\
        x_{in}
    \end{pmatrix}\times  \Big((1-(\delta_{\mu_i 0}+\delta_{\mu_i k-1}))\frac{(\textup{erf'}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf'}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}{ (\textup{erf}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}+~\\
    +(\delta_{\mu_i 0}-\delta_{\mu_i k-1})\frac{\textup{erf'}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma})}{(1+\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}\Big) .
\end{multline}
$\delta_{ij} = \{1, i=j; 0, i\neq j\}$ - символ Кронекера, $0_{n+1}=(0, \dots, 0)^T$ - вектор размерности $n+1$ состоящий из одних нулей.

В полученном уравнении используется функция $\textup{erf}(x)$ и ее производная. Для вычисления значений этой фунции существует таблица значений и приближение аналитической функцией \cite{Winitzki}.
Будем использовать приближение для функции $\textup{erf}(x)$:
\begin{eqnarray}
    \label{eq25}(\textup{erf} (x))^2&\approx& 1- \exp(-x^2 \frac{\frac{4}{\pi}+ax^2}{1+ax^2}),\\
    \nonumber a&=&\frac{8}{3\pi}\frac{3-\pi}{\pi -4}.
\end{eqnarray}
Оно считается достаточно точным для $x$ близких к $0$ и к $\infty$ \cite{Winitzki}. \hfill\break

Найдем производную для этого приближения:
\begin{eqnarray}
    \label{eq26}\textup{erf}'(x) = \exp(-x^2 \frac{\frac{4}{\pi}+ax^2}{1+ax^2}) \frac{-2x\frac{\frac{4}{\pi}+ax^2}{1+ax^2}+(2ax^3)\frac{\frac{4}{\pi}+ax^2}{1+ax^2}-\frac{2ax^3}{1+ax^2}}{2\sqrt{1- \exp(-x^2 \frac{\frac{4}{\pi}+ax^2}{1+ax^2})}}.
\end{eqnarray}

Подставив  выражения (\ref{eq25})-(\ref{eq26}) в (\ref{eq24}), получим уравнение, которое будем решать методом секущих.


\subsubsection{Метод секущих}\label{sec4_2}
Так как мы не можем привести систему $ \frac{\delta l}{\delta \beta}=0$ к виду, удобному для итерации, то нам придется искать ее нули с помощью метода секущих \cite{NumericalMethods}.
Введем вектор ошибки $\check{\varepsilon}^{(k)}=\beta^{*}-\beta^{(k)}$. Тогда для определения вектора ошибки имеем:
\begin{eqnarray}
    \frac{\delta l (\beta^{(k)}+\check{\varepsilon}^{(k)})}{\delta \beta}=0.
\end{eqnarray}
Строя разложение левой части по формуле Тейлора и ограничиваясь лишь линейными членами\cite{NumericalMethods}, будем иметь систему:
\begin{eqnarray}
    \frac{\delta }{\delta \beta}\frac{\delta l (\beta^{(k)})}{\delta \beta}\Delta \beta^{(k)}=-\frac{\delta l (\beta^{(k)})}{\delta \beta}.
\end{eqnarray}
Вторая производная функции $l$ приближается с помощью выражения:
\begin{eqnarray}
    \frac{\delta }{\delta \beta_j}\frac{\delta l(\beta_1^{(k)},\dots, \beta_n^{(k)}) }{\delta \beta}\approx \frac{\frac{\delta l(\beta_1^{(k)},\dots,\beta_j^{(k)},\dots, \beta_n^{(k)}) (\beta^{(k)}}{\delta \beta}-\frac{\delta l(\beta_1^{(k)},\dots,\beta_j^{(k-1)},\dots, \beta_n^{(k)}) (\beta^{(k)}}{\delta \beta}}{\beta_j^{(k)}-\beta_j^{(k-1)}}.
\end{eqnarray}
Если матрица $\frac{\delta }{\delta \beta}\frac{\delta l (\beta^{(k)}}{\delta \beta}$ невырожденная (а в нашем случае она диагональная), то из этой системы можно единственным образом найти $\Delta \beta^{(k)}$ и построить приближение:
\begin{eqnarray}
    \beta^{(k+1)}=\beta^{(k)}+\Delta \beta^{(k)}.
\end{eqnarray}

Согласно \cite{NumericalMethods} метод секущих в общем случае обладает квадратичной скоростью сходимости.

Одним из минусов метода секущих является то, что он требует два начальных приближения, т.е. отрезок, где, как мы предполагаем, находится точное значение параметров регрессии.
Условием остановки метода секущих будем задавать такое, что разница значений логарифмической функции правдоподобия $l$ на двух соседних итерациях не более некоторой заданной величины.
Теперь имеем нули производной функции $l$, а также ее значения на границе отрезка $[a,b]$.
Переберем эти значения и таким образом найдем значение вектора $\hat{\beta}$, где она достигает своего максимального значения.

\subsection{Переклассификация выборки}
Отметим, что в исходной постановке задачи данные содержат аномальные наблюдения, которые тоже подвергаются группированию. 
Наличие аномальных наблюдений в исходных данных портит точность метода максимального правдоподобия (см. раздел \ref{ss_4_9}).
Для уменьшения влияния выбросов будем использовать переклассификацию выборки.
Идея заключается в том, что аномальные наблюдения с большей вероятностью попадают не в те интервалы, в которые попадают истинные наблюдения. 
При этом переклассификация может помочь отнести аномальные наблюдения к истинным классам и улучшить качество оценивания.


В ходе выполнения дипломной работы было испробовано несколько способов переклассификации выборки. Один из них: метод $K$-ближайших соседей. 

\subsubsection{Метод $K$-ближайших соседей}
На первом этапе для каждого вектора $x_i$ задан класс $\mu_i$: т.е. пара $(x_i,\mu_i)$.
Далее выполним переклассификацию выборки. 
Для этого построим новую выборку такого же объема $N$.
Пройдемся по каждому элементу $(x_i, \mu_i)$ выборки и для этого наблюдения построим новое:
\begin{eqnarray}
    (x_i, \check{\mu}_i),
\end{eqnarray}
где $\check{\mu}_i$ получен по методу $K$-ближайших соседей\cite{NEAREST_NEIGHBOR}.\hfill\break
\begin{eqnarray}
    \check{\mu}_i = \arg\max_j \sum_{k \in V_i,~k\neq i} \delta_{\check{\mu}_k j}~,
\end{eqnarray}
где $V_i$ множество индексов $l$ первых $K$ векторов $x_l$, отсортированных по возрастанию расстояния до вектора $x_i$.

После переклассификации выборки, к ней применяется функция правдоподобия (\ref{eq22}), только теперь с использованием новых классов $\check{\mu}_i$ вместо $\mu_i$. 
Она аналогично максимизируется и в итоге находится новая оценка параметров $\hat{\beta}$.

В ходе экспериментов (раздел \ref{sec_exp}) оказалось, что метод $K$ ближайших соседей исправляет очень мало ошибочных классов в выборке, поэтому было решено использовать другой метод переклассификации.

\subsubsection{Переклассификация с использованием Локального уровня выброса и Случайного леса}
Идея метода заключается в следующем: определим в выборке такие наблюдения, которые, вероятно, являются аномальными с помощью какого-либо способа. После этого выберем наблюдения, которые не определились как аномальные.
Обучим на полученных наблюдениях классификатор. После этого с помощью обученного классификатора переклассифицируем те наблюдения, которые определились как аномальные. В итоге получим выборку, которую будем использовать при решении уравнения (\ref{eq22}) \cite{LOF}.

Для определения аномальных наблюдений можно использовать Локальный уровень выброса. Метод был предложен Маркусом М.~Бройнигом, Гансом-Петер~Кейгелем, Реймондом~Т.~НГ и Ёргом Сандером в 2000 году. Аномальные наблюдения находятся с помощью измерения локального отклонения точек с учётом их соседей.
Метод основан на концепте локальной плотности достижимости, где локальная плотность достижимости вычисляется с учётом ближайших $K$ соседей, расстояние до которых используется для вычисления плотности. Сравнивая локальную плотность точки с локальной плотностью соседей можно определять точки, которые обладают значительно меньшей локальной плотностью по сравнению с соседями. Такие точки будем считать выбросами.

Пусть $x_i$ является некоторой точкой выборки. Определим $\rho(x_i, x_l)$ -- расстояние от точки $x_i$ до точки $x_l$ (будем использовать Евклидову метрику), $\rho_k(x_i)$ -- расстояние от точки $x_i$ до её $K$-го соседа. 
Аналогично методу $K$-ближайших соседей обозначим множество индексов $k$ первых $K$ векторов $x_k$, отсортированных по возрастанию расстояния до вектора $x_i$ через $V_i$.
Использользуя введеные величины зададим:
\begin{eqnarray}
    \hat{\rho}_k(x_i, x_l) = \max\{\rho_k(x_l), \rho(x_i, x_l)\}.
\end{eqnarray}

Такую величину можно интерпретировать как истинное расстояние от точки $x_l$ до точки $x_i$ если $x_l$ не входит в ее $K$ ближайших соседей или расстояние до точки $x_{V_{l_K}}$ в противном случае. 
Данная величина введена для того, чтобы более стабильно вычислять расстояние для тех значений, которые входят в $K$-ближайших соседей точки $x_l$. 

Заметим, что введенная величина не обладает свойством симметричности.

Локальную плотность достижимости зададим выражением:
\begin{eqnarray}
    \textup{lrd}_k(x_i) = 1 / \bigg(\frac{\sum\limits_{l\in V_i}\hat{\rho}_k(x_i, x_l)}{|V_i|}\bigg).
\end{eqnarray}

Теперь можно задать величину:
\begin{eqnarray}
    \textup{LOF}_k(x_i) = \frac{\sum\limits_{l\in V_i}\frac{\textup{lrd}_k(x_l)}{\textup{lrd}_k(x_i)}}{|V_i|}.
\end{eqnarray}
Она является средней локальной плотностью достижимости соседей точки $x_i$ по отношению к своей локальной плотности достижимости точки $x_i$. 

По полученному значению можно судить \cite{LOF}, является ли наблюдение $x_i$ выбросом или нет:
\begin{itemize}
    \item $\textup{LOF}_k(x_i)\sim 1$   -- означает, что у наблюдения $x_i$ такая же плотность как и у его соседей;\\
    \item $\textup{LOF}_k(x_i)<1$  -- означает, что у наблюдения $x_i$ большая плотность чем у его соседей (наблюдение - истинное);\\
    \item $\textup{LOF}_k(x_i)>1$ -- означает, что у наблюдения $x_i$ меньшая плотность чем у его соседей (наблюдение - выброс).
\end{itemize}

Таким образом найдем аномальные наблюдения в выборке.

Теперь обучим классификатор на предполагаемых истинных значениях выборки. Воспользуемся алгоритмом классификации на основе Случайного леса \cite{RANDOM_FORESTS}. Построим по выборке несколько деревьев решений. 
Применим обученный классификатор к каждому аномальному наблюдению. Тот класс, к которому отнесло наибольшее количество построенных деревьев решений будем считать истинным классом данного аномального наблюдения.

В ходе компьютерных экспериментов была построена таблица подходящих значений $K$ для заданной длины интервала и объема выборки (раздел \ref{ss3_3_2} таблица \ref{tab2}).
В целом, данный метод показал хорошие результаты и может быть использован на практике для улучшения результатов оценивания методом максимального правдоподобия.

\subsection{Альтернативные оценки параметров модели}
Рассмотрим альтернативный метод оценивания параметров модели линейной регрессии с выбросами при наличии группирования наблюдений, основанный на замене группированных наблюдений серединами соответствующих интервалов. 
Такой метод встречается в литературе, например в \cite{interval_valued}.

Метод заключается в следующем:
пусть имеется $\mu_i$ - номер полуинтервала, в который попало очередное наблюдение $y_i$. Ему соответствует полуинтервал $\nu_{\mu_i}$ (из (\ref{eq13})), т.е. полуинтервал:
\begin{eqnarray}
    y_i\in (a_{\nu_{\mu_i}},a_{\nu_{\mu_i}+1}], i = \overline{1,N}
\end{eqnarray}
(считаем что $a_1<y_i<a_{k-1}, i=\overline{1,N}$, т.е $1\leq\mu_i\leq k-2$).

Найдем центральную точку этого интервала, т.е. точку
\begin{eqnarray}
    \check{y_i} = \frac{a_{\nu_{\mu_i}} + a_{\nu_{\mu_i}+1}}{2}.
\end{eqnarray}

Построим для всех значений функции регрессии $y_i$ значения $\check{y_i}$.
Будем использовать в качестве значений функции регрессии полученные значения $\check{y_i}$, а в качестве регрессоров $x_i$ и построим МНК оценки параметров $\beta$.

Теперь имеет три вида оценок: оценки максимального правдоподобия, оценки максимального правдоподобия с переклассификацией, МНК по серединам интервалов. 
В разделе \ref{ss_4_6} было проведено сравнение этих трех методов. 
Оценки максимального правдоподобия с переклассификацией показали самые лучшие результаты.
\subsection{Полиномиальная регрессия}
Введем теперь модель полиномиальной регрессии.

\begin{equation}
    \begin{array}{c}
        \label{eq28}y_i=\beta_0+\beta_1 x_{i1}^1+\beta_2 x_{i2}^2+\dots+\beta_n x_{in}^n+\varepsilon_i, i=\overline{1,N},\\
        y_i = \sum\limits_{l=1}^{n} x_{il}^{l-1} + \varepsilon_i, i=\overline{1,N},\\
        y_i= f(x_i,\beta)+\varepsilon_i,\\
        f(x_i,\beta)=\beta_0+\beta_1 x_{i1}^1+\beta_2 x_{i2}^2+\dots+\beta_n x_{in}^n.
    \end{array}
\end{equation}
В случае полиномиальной регрессии также справедливо:
\begin{eqnarray}
    y_i\sim \mathcal{N}(f(x_i,\beta),\sigma^2).
\end{eqnarray}

Поскольку оценки строились путём максимизирования функции:
\begin{eqnarray}
    &l(\beta,\sigma^2, \nu_0,\dots, \nu_{k-1})=\sum_{i=1}^{N}\ln(P(y_i\in \nu_{\mu_i}))=\\
    &=\sum\limits_{i=1}^{N}\ln\begin{cases}
        \frac{1}{2}(\textup{erf}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma})),~\mu_i=\overline{1,k-2},\\
        \frac{1}{2}(1+\textup{erf}(\frac{a_{1}-f(x_i,\beta)}{\sqrt{2}\sigma})),~\mu_i=0,\\
        \frac{1}{2}(1+\textup{erf}(\frac{a_{k-1}-f(x_i,\beta)}{\sqrt{2}\sigma})),~\mu_i=k-1,
    \end{cases}
\end{eqnarray}
а функция правдоподобия максимизировалась путём решения системы уравнений:
\begin{eqnarray}
    \frac{\delta l}{\delta \beta}=0,
\end{eqnarray}
которая примет вид:
\begin{multline}
    \frac{\delta l}{\delta \beta}=\frac{\delta \sum\limits_{i=1}^{N}\ln P(y_i\in \nu_{\mu_i})}{\delta \beta}=~\\
    =\frac{\delta \sum\limits_{i=1}^{N} \ln(\frac{1}{2}(\textup{erf}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma})) )         }{\delta \beta}=~\\
    =  \sum\limits_{i=1}^{N}\Big((1-(\delta_{\mu_i 0}+\delta_{\mu_i k-1}))\frac{(\textup{erf'}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf'}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}{ (\textup{erf}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}+~\\
    +(\delta_{\mu_i 0}-\delta_{\mu_i k-1})\frac{\textup{erf'}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma})}{(1+\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}\Big)  (-1) \frac{\delta f(x_i,\beta)}{\delta \beta} )=~
\end{multline}
\begin{multline}
    \nonumber 
    =-\sum_{i=1}^{N}\begin{pmatrix}
        1\\
        x_{i1}^1\\
        \dots\\
        x_{in}^n
    \end{pmatrix}\times  \Big((1-(\delta_{\mu_i 0}+\delta_{\mu_i k-1}))\frac{(\textup{erf'}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf'}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}{ (\textup{erf}(\frac{a_{\mu_i+1}-f(x_i,\beta)}{\sqrt{2}\sigma})-\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}+~\\
    +(\delta_{\mu_i 0}-\delta_{\mu_i k-1})\frac{\textup{erf'}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma})}{(1+\textup{erf}(\frac{a_{\mu_i}-f(x_i,\beta)}{\sqrt{2}\sigma}))}\Big),
\end{multline}
то построенные оценки также применимы и для полиномиальной регрессии.

Также как и в случае линейной регрессии считаем, что выборка содержит выбросы, т.е., аналогично:
\begin{eqnarray}
    y_i^{\widetilde{\varepsilon}}=(\xi_i)y_i+ (1-\xi_i)\eta_i,
\end{eqnarray}
здесь $y_i$ задаются формулой (\ref{eq28}).

